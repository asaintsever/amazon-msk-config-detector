=================================== Beginning of the report ===================================
Cluster arn:arn:aws:kafka:ap-southeast-2:1111:cluster/mskt1/49c015aa-2f11-111-b5d8-2c11123d-3
Instance type:kafka.m5.4xlarge
Number of brokers:3
Storage mode:TIERED
Number of AZs:3 (Cluster is deployed across three AZs and following the best practices of AZ deployment.)
**Current provisioned throughput for each borker is:400
**The recommended provisioned throughput for each kafka.m5.4xlarge broker is:593

************ Analyzing cluster config ************
Current cluster configuration:
num.io.threads=8
num.network.threads=5
num.replica.fetchers=2
replica.lag.time.max.ms=30000
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
socket.send.buffer.bytes=102400

The recommended (or default if not specified) configuration for kafka.m5.4xlarge broker type are as below:
num.io.threads:16
num.network.threads:8
remote.log.reader.threads:10
log.segment.bytes:134217728
replica.lag.time.max.ms:30000
socket.receive.buffer.bytes:102400
socket.request.max.bytes:104857600
socket.send.buffer.bytes:102400
num.replica.fetchers:4

**Update cluster configuration to recommended value if different from custom configuration. Leave it to default if none specified.

************ Analyzing partition count ************
*Recommended partition count for each kafka.m5.4xlarge broker type is:4000
Current partition count for each broker is: 
Broker ID: 1| Partition count:557
Broker ID: 2| Partition count:557
Broker ID: 3| Partition count:557
*Total partitions (including replica) in this cluster are: 1671

************ Analyzing disk usage ************
Broker ID: 1| Disk usage :2.461628%
Broker ID: 2| Disk usage :2.74686%
Broker ID: 3| Disk usage :2.307561%

************ Analyzing topics ************
Total number of topics:7
List of topic:
['ts2-gp3', 'testCMP', '__consumer_offsets', 'testCMP1', 'ts1-gp3']

Topic Config violation for topic: ts2-gp3
Current config= replicationfactor: 2,segment.ms:604800000,min.insync.replicas:2,segment.bytes:500000000
Recommended config= replicationfactor: 3,segment.ms:604800000,min.insync.replicas:2,segment.bytes:134217728

Topic Config violation for topic: __consumer_offsets
Current config= replicationfactor: 3,segment.ms:604800000,min.insync.replicas:2,segment.bytes:104857600
Recommended config= replicationfactor: 3,segment.ms:604800000,min.insync.replicas:2,segment.bytes:134217728

Topic Config violation for topic: ts1-gp3
Current config= replicationfactor: 2,segment.ms:604800000,min.insync.replicas:2,segment.bytes:500000000
Recommended config= replicationfactor: 3,segment.ms:604800000,min.insync.replicas:2,segment.bytes:134217728

************ General configuration best practices ************

1. Replica factor=1 causes offline partitions during maintanance of cluster, which prevents client to produce/consume messeages

2. segment.ms -Wrong configuration could lead to cluster downtime. For example if segment.ms = 20000, it would causes Kafka to generate a new log segment every 20000 milliseconds instead of once every 7 days (or 604800000 ms) which is the default value. Kafka stores two memory mapped files per log segment. This configuration will generate a high number of memory mapped files and Kafka is approaching the max file limit per Linux process of 262,144.

3. min.insync.replicas should be set to a value equal to or less than the replication factor. Failing to do so could increase the risk of availability loss.

4. Changing max.message.bytes configuration with large number causes brokers to receive too large requests and cannot allocate the memory to read it in user space. Therefore, brokers can become unhealthy. Leave it to default.

5. It is recommended to keep the default value of segment.bytes (1GB) to avoid generating too many memory mapped files, which may cause Kafka to approach the maximum file limit per Linux process and result in broker unavailability.
=================================== End of the report ===================================
